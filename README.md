## Short Notes

- [Standard Transformer](https://github.com/paulosantosneto/transformers-variants-playground/notes/)
- [PostLN, PreLN and ResiDual Transformer](https://github.com/paulosantosneto/transformers-variants-playground/notes/)
- [Multi-query and Multi-group](https://github.com/paulosantosneto/transformers-variants-playground/notes/)

## Applications

- [Machine Translation](https://github.com/paulosantosneto/transformers-variants-playground/notes/)

## References

### Foundations

[1] Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017). Attention Is All You Need. 

### About Normalization

[2] Xiong, Ruibin & Yang, Yunchang & He, Di & Zheng, Kai & Shuxin, Zheng & Xing, Chen & Zhang, Huishuai & Lan, Yanyan & Wang, Liwei & Liu, Tie-Yan. (2020). On Layer Normalization in the Transformer Architecture. 

[3] Liu, Liyuan & Liu, Xiaodong & Gao, Jianfeng & Chen, Weizhu & Han, Jiawei. (2020). Understanding the Difficulty of Training Transformers. 5747-5763. 10.18653/v1/2020.emnlp-main.463. 

### About Multi-head Attention

[4] Shazeer, Noam. (2019). Fast Transformer Decoding: One Write-Head is All You Need. 

[5] Ainslie, Joshua & Lee-Thorp, James & Jong, Michiel & Zemlyanskiy, Yury & Lebr√≥n, Federico & Sanghai, Sumit. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. 

