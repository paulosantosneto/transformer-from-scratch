## Short Notes

Short Notes contain mathematical insights and basics concepts related to the components of variants transformer architectures.

- [Foundations of Transformers](https://github.com/paulosantosneto/transformer-variants/blob/main/notes/Foundations.md)
- [PostLN, PreLN and ResiDual Transformer](https://github.com/paulosantosneto/transformer-variants/blob/main/notes/AboutNormalization.md)
- [Multi-query and Multi-group](https://github.com/paulosantosneto/transformer-variants/blob/main/notes/AboutMultihead.md)

## Applications

- [Machine Translation](https://github.com/paulosantosneto/transformer-variants/blob/main/notes/MachineTranslation.md)

## References

### Foundations

[1] Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017). Attention Is All You Need. 

[2] Müller, Rafael & Kornblith, Simon & Hinton, Geoffrey. (2019). When Does Label Smoothing Help?. 

[3] Sennrich, Rico & Haddow, Barry & Birch, Alexandra. (2015). Neural Machine Translation of Rare Words with Subword Units. 

## Optimization

[3] Howard, Jeremy & Ruder, Sebastian. (2018). Universal Language Model Fine-tuning for Text Classification. 328-339. 10.18653/v1/P18-1031. 

[3] Zhang, Tianyi & Wu, Felix & Katiyar, Arzoo & Weinberger, Kilian & Artzi, Yoav. (2020). Revisiting Few-sample BERT Fine-tuning. 

### About Normalization

[3] Xiong, Ruibin & Yang, Yunchang & He, Di & Zheng, Kai & Shuxin, Zheng & Xing, Chen & Zhang, Huishuai & Lan, Yanyan & Wang, Liwei & Liu, Tie-Yan. (2020). On Layer Normalization in the Transformer Architecture. 

[4] Liu, Liyuan & Liu, Xiaodong & Gao, Jianfeng & Chen, Weizhu & Han, Jiawei. (2020). Understanding the Difficulty of Training Transformers. 5747-5763. 10.18653/v1/2020.emnlp-main.463. 

### About Multi-head Attention

[5] Shazeer, Noam. (2019). Fast Transformer Decoding: One Write-Head is All You Need. 

[6] Ainslie, Joshua & Lee-Thorp, James & Jong, Michiel & Zemlyanskiy, Yury & Lebrón, Federico & Sanghai, Sumit. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. 

## Evaluation

[7] Papineni, Kishore & Roukos, Salim & Ward, Todd & Zhu, Wei Jing. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation. 10.3115/1073083.1073135. 

